{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import array\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.text as kpt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo se realizará la predicción utilizando solo el texto del dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train_processed.csv')\n",
    "test_df = pd.read_csv('../Data/test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo los datos y los labels.\n",
    "train_x = np.asarray(train_df['text_clean'])\n",
    "test_x = np.asarray(test_df['text_clean'])\n",
    "train_y = np.asarray(train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo trabajo con las 3000 palabras mas populares del set de datos.\n",
    "palabras_maximas = 3000\n",
    "\n",
    "# Defino una función que me procese los datos.\n",
    "def procesarDatos(arrayDeTextos):\n",
    "    # Creo el tokenizador.\n",
    "    tokenizador = Tokenizer(num_words = palabras_maximas)\n",
    "\n",
    "    # Le paso los tweets al tokenizer.\n",
    "    tokenizador.fit_on_texts(arrayDeTextos)\n",
    "\n",
    "    # Lista de palabras y de indices que me da el tokenizer.\n",
    "    diccionario = tokenizador.word_index    \n",
    "\n",
    "    tweetsComoIndicesDePalabras = []\n",
    "\n",
    "    # Función que convierte los tweets a un vector de indices.\n",
    "    for tweet in arrayDeTextos:\n",
    "        indicesDePalabrasDelTweet = [diccionario[palabra] for palabra in kpt.text_to_word_sequence(tweet)]\n",
    "        tweetsComoIndicesDePalabras.append(indicesDePalabrasDelTweet)\n",
    "    \n",
    "    # Paso a array de numpy.\n",
    "    tweetsComoIndicesDePalabras = np.asarray(tweetsComoIndicesDePalabras)\n",
    "\n",
    "    # Creo una matriz del tipo one-hot encoding.\n",
    "    matriz_x = tokenizer.sequences_to_matrix(tweetsComoIndicesDePalabras, mode='binary')\n",
    "\n",
    "    return matriz_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-62c630f8048a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_matriz_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesarDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_matriz_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocesarDatos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# La lista de labels es una lista de categorias: 1 tweet real; 0 tweet no real.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-632042febfe6>\u001b[0m in \u001b[0;36mprocesarDatos\u001b[0;34m(arrayDeTextos)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Creo una matriz del tipo one-hot encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmatriz_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msequences_to_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweetsComoIndicesDePalabras\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatriz_x\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "train_matriz_x = procesarDatos(train_x)\n",
    "test_matriz_x = procesarDatos(test_x)\n",
    "\n",
    "# La lista de labels es una lista de categorias: 1 tweet real; 0 tweet no real.\n",
    "train_y = tf.keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el modelo sequencial de tensorflow utilizando keras.\n",
    "\n",
    "# HIPERPARÁMETROS A TENER EN CUENTA:\n",
    "# 1 - Cantidad de palabras máximas.\n",
    "# 2 - Número de salida del primer layer.\n",
    "# 3 - Función de activación del primer layer.\n",
    "# 4 - El porcentage de desactivación del primer dropout.\n",
    "# 5 - Número de salida del segundo layer.\n",
    "# 6 - Función de activación del segundo layer.\n",
    "# 7 - El porcentage de desactivación del segundo dropout.\n",
    "# 8 - Función de activación del layer final.\n",
    "# 9 - La función loss del modelo.\n",
    "# 10 - La función optimizer del modelo.\n",
    "\n",
    "\n",
    "# Red neuronal que es un stack de layer ejecutados en orden.\n",
    "modelo = Sequential() \n",
    "\n",
    "# El primer y último layer son de los más importantes ya que son la entrada y la salida.\n",
    "# El primer layer tiene entrada de 3000 que es la cantidad máxima de palabras.\n",
    "# 512 es la salida de este layer y la función de activación/maximización es relu.\n",
    "modelo.add(Dense(512, input_shape=(palabras_maximas,), activation='relu'))\n",
    "\n",
    "# Desactivo neuronas con el fin de no caer en overfitting.\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "# Capa intermedia que tiene como salida 256 neuronas y la función de activación es sigmoid.\n",
    "modelo.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "# Desactivo neuronas con el fin de no caer en overfitting.\n",
    "modelo.add(Dropout(0.2))\n",
    "\n",
    "# Layer de salida, tiene solo 2 que la cantidad de posibilidades que tiene el target.\n",
    "# Su función de activación es softmax.\n",
    "modelo.add(Dense(2, activation='softmax'))\n",
    "\n",
    "# Finalmente compilo el modelo.\n",
    "# Cómo métrica le pido que me devuelva el accuracy, la precisión y el recall \n",
    "# para poder calcular con estas últimas dos la puntuación F1.\n",
    "modelo.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entreno el modelo.\n",
    "\n",
    "# HIPERPARÁMETROS A TENER EN CUENTA:\n",
    "# 1 - Batch_size.\n",
    "# 2 - Epochs.\n",
    "# 3 - Split.\n",
    "\n",
    "# Entro el modelo tomando los datos de a 32 registros.\n",
    "# Hago 20 iteraciones.\n",
    "# Valido con el 20% de los datos.\n",
    "modelo.fit(matriz_x, train_y, batch_size = 32, epochs = 20, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicciones = modelo.predict(test_matriz_x)\n",
    "categorias = array.array('i')\n",
    "\n",
    "for prediccion in predicciones:\n",
    "    categorias.append(np.argmax(prediccion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genero un dataframe de pandas y lo guardo.\n",
    "test_prediccion = test_df['id'].to_frame()\n",
    "test_prediccion['target'] = pd.Series(categorias)\n",
    "test_prediccion.to_csv('tensorFlow.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
