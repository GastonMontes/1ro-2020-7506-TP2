{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import array\n",
    "import json\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.preprocessing.text as kpt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para este ejemplo se realizará la predicción utilizando solo el texto del dataset original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../Data/train_processed.csv')\n",
    "test_df = pd.read_csv('../Data/test_processed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo los datos y los labels.\n",
    "train_x = np.asarray(train_df['text_clean'])\n",
    "train_y = np.asarray(train_df['target'])\n",
    "\n",
    "test_x = np.asarray(test_df['text_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo trabajo con las 3000 palabras mas populares del set de datos.\n",
    "palabras_maximas = 3000\n",
    "\n",
    "# Defino una función que me procese los datos.\n",
    "def procesarDatos(arrayDeTextos):\n",
    "    # Creo el tokenizador.\n",
    "    tokenizador = Tokenizer(num_words = palabras_maximas)\n",
    "\n",
    "    # Le paso los tweets al tokenizer.\n",
    "    tokenizador.fit_on_texts(arrayDeTextos)\n",
    "\n",
    "    # Lista de palabras y de indices que me da el tokenizer.\n",
    "    diccionario = tokenizador.word_index    \n",
    "\n",
    "    tweetsComoIndicesDePalabras = []\n",
    "\n",
    "    # Función que convierte los tweets a un vector de indices.\n",
    "    for tweet in arrayDeTextos:\n",
    "        indicesDePalabrasDelTweet = [diccionario[palabra] for palabra in kpt.text_to_word_sequence(tweet)]\n",
    "        tweetsComoIndicesDePalabras.append(indicesDePalabrasDelTweet)\n",
    "    \n",
    "    # Paso a array de numpy.\n",
    "    tweetsComoIndicesDePalabras = np.asarray(tweetsComoIndicesDePalabras)\n",
    "\n",
    "    # Creo una matriz del tipo one-hot encoding.\n",
    "    matriz_x = tokenizador.sequences_to_matrix(tweetsComoIndicesDePalabras, mode='binary')\n",
    "\n",
    "    return matriz_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matriz_x = procesarDatos(train_x)\n",
    "test_matriz_x = procesarDatos(test_x)\n",
    "\n",
    "# La lista de labels es una lista de categorias: 1 tweet real; 0 tweet no real.\n",
    "train_y = tf.keras.utils.to_categorical(train_y, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creo el modelo sequencial de tensorflow utilizando keras.\n",
    "\n",
    "# HIPERPARÁMETROS A TENER EN CUENTA:\n",
    "# 1 - Cantidad de palabras máximas.\n",
    "# 2 - Número de salida del primer layer.\n",
    "# 3 - Función de activación del primer layer.\n",
    "# 4 - El porcentage de desactivación del primer dropout.\n",
    "# 5 - Número de salida del segundo layer.\n",
    "# 6 - Función de activación del segundo layer.\n",
    "# 7 - El porcentage de desactivación del segundo dropout.\n",
    "# 8 - Función de activación del layer final.\n",
    "# 9 - La función loss del modelo.\n",
    "# 10 - La función optimizer del modelo.\n",
    "\n",
    "def crearModelo():\n",
    "    # Red neuronal que es un stack de layer ejecutados en orden.\n",
    "    modelo = Sequential() \n",
    "\n",
    "    # El primer y último layer son de los más importantes ya que son la entrada y la salida.\n",
    "    # El primer layer tiene entrada de 3000 que es la cantidad máxima de palabras.\n",
    "    # 512 es la salida de este layer y la función de activación/maximización es relu.\n",
    "    modelo.add(Dense(512, input_shape=(palabras_maximas,), activation='relu'))\n",
    "\n",
    "    # Desactivo neuronas con el fin de no caer en overfitting.\n",
    "    modelo.add(Dropout(0.2))\n",
    "\n",
    "    # Capa intermedia que tiene como salida 256 neuronas y la función de activación es sigmoid.\n",
    "    modelo.add(Dense(256, activation='sigmoid'))\n",
    "\n",
    "    # Desactivo neuronas con el fin de no caer en overfitting.\n",
    "    modelo.add(Dropout(0.2))\n",
    "\n",
    "    # Layer de salida, tiene solo 2 que la cantidad de posibilidades que tiene el target.\n",
    "    # Su función de activación es softmax.\n",
    "    modelo.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Finalmente compilo el modelo.\n",
    "    # Cómo métrica le pido que me devuelva el accuracy, la precisión y el recall \n",
    "    # para poder calcular con estas últimas dos la puntuación F1.\n",
    "    modelo.compile(loss='categorical_crossentropy', optimizer='adam', metrics = ['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])\n",
    "    \n",
    "    return modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "160/160 [==============================] - 3s 18ms/step - loss: 0.6611 - accuracy: 0.6214 - precision: 0.6214 - recall: 0.6214 - val_loss: 0.4801 - val_accuracy: 0.7676 - val_precision: 0.7676 - val_recall: 0.7676\n",
      "Epoch 2/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.3102 - accuracy: 0.8770 - precision: 0.8770 - recall: 0.8770 - val_loss: 0.4815 - val_accuracy: 0.7791 - val_precision: 0.7791 - val_recall: 0.7791\n",
      "Epoch 3/20\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.1861 - accuracy: 0.9363 - precision: 0.9363 - recall: 0.9363 - val_loss: 0.5732 - val_accuracy: 0.7549 - val_precision: 0.7549 - val_recall: 0.7549\n",
      "Epoch 4/20\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.1185 - accuracy: 0.9620 - precision: 0.9620 - recall: 0.9620 - val_loss: 0.6317 - val_accuracy: 0.7481 - val_precision: 0.7481 - val_recall: 0.7481\n",
      "Epoch 5/20\n",
      "160/160 [==============================] - 2s 14ms/step - loss: 0.1032 - accuracy: 0.9691 - precision: 0.9691 - recall: 0.9691 - val_loss: 0.6886 - val_accuracy: 0.7561 - val_precision: 0.7561 - val_recall: 0.7561\n",
      "Epoch 6/20\n",
      "160/160 [==============================] - 2s 15ms/step - loss: 0.0783 - accuracy: 0.9732 - precision: 0.9732 - recall: 0.9732 - val_loss: 0.7276 - val_accuracy: 0.7537 - val_precision: 0.7537 - val_recall: 0.7537\n",
      "Epoch 7/20\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0810 - accuracy: 0.9684 - precision: 0.9684 - recall: 0.9684 - val_loss: 0.7687 - val_accuracy: 0.7493 - val_precision: 0.7493 - val_recall: 0.7493\n",
      "Epoch 8/20\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0672 - accuracy: 0.9758 - precision: 0.9758 - recall: 0.9758 - val_loss: 0.7804 - val_accuracy: 0.7493 - val_precision: 0.7493 - val_recall: 0.7493\n",
      "Epoch 9/20\n",
      "160/160 [==============================] - 2s 13ms/step - loss: 0.0654 - accuracy: 0.9738 - precision: 0.9738 - recall: 0.9738 - val_loss: 0.8040 - val_accuracy: 0.7334 - val_precision: 0.7334 - val_recall: 0.7334\n",
      "Epoch 10/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0582 - accuracy: 0.9794 - precision: 0.9794 - recall: 0.9794 - val_loss: 0.8000 - val_accuracy: 0.7413 - val_precision: 0.7413 - val_recall: 0.7413\n",
      "Epoch 11/20\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.0520 - accuracy: 0.9819 - precision: 0.9819 - recall: 0.9819 - val_loss: 0.7975 - val_accuracy: 0.7362 - val_precision: 0.7362 - val_recall: 0.7362\n",
      "Epoch 12/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0513 - accuracy: 0.9792 - precision: 0.9792 - recall: 0.9792 - val_loss: 0.8658 - val_accuracy: 0.7533 - val_precision: 0.7533 - val_recall: 0.7533\n",
      "Epoch 13/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0544 - accuracy: 0.9785 - precision: 0.9785 - recall: 0.9785 - val_loss: 0.8368 - val_accuracy: 0.7441 - val_precision: 0.7441 - val_recall: 0.7441\n",
      "Epoch 14/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0613 - accuracy: 0.9726 - precision: 0.9726 - recall: 0.9726 - val_loss: 0.8368 - val_accuracy: 0.7354 - val_precision: 0.7354 - val_recall: 0.7354\n",
      "Epoch 15/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0458 - accuracy: 0.9800 - precision: 0.9800 - recall: 0.9800 - val_loss: 0.8348 - val_accuracy: 0.7382 - val_precision: 0.7382 - val_recall: 0.7382\n",
      "Epoch 16/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0522 - accuracy: 0.9785 - precision: 0.9785 - recall: 0.9785 - val_loss: 0.8655 - val_accuracy: 0.7274 - val_precision: 0.7274 - val_recall: 0.7274\n",
      "Epoch 17/20\n",
      "160/160 [==============================] - 2s 11ms/step - loss: 0.0508 - accuracy: 0.9786 - precision: 0.9786 - recall: 0.9786 - val_loss: 0.8424 - val_accuracy: 0.7326 - val_precision: 0.7326 - val_recall: 0.7326\n",
      "Epoch 18/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0473 - accuracy: 0.9781 - precision: 0.9781 - recall: 0.9781 - val_loss: 0.8620 - val_accuracy: 0.7342 - val_precision: 0.7342 - val_recall: 0.7342\n",
      "Epoch 19/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0566 - accuracy: 0.9751 - precision: 0.9751 - recall: 0.9751 - val_loss: 0.8721 - val_accuracy: 0.7338 - val_precision: 0.7338 - val_recall: 0.7338\n",
      "Epoch 20/20\n",
      "160/160 [==============================] - 2s 12ms/step - loss: 0.0479 - accuracy: 0.9798 - precision: 0.9798 - recall: 0.9798 - val_loss: 0.9105 - val_accuracy: 0.7159 - val_precision: 0.7159 - val_recall: 0.7159\n"
     ]
    }
   ],
   "source": [
    "# Entreno el modelo.\n",
    "\n",
    "# HIPERPARÁMETROS A TENER EN CUENTA:\n",
    "# 1 - Batch_size.\n",
    "# 2 - Epochs.\n",
    "# 3 - Split.\n",
    "\n",
    "# Entro el modelo tomando los datos de a 32 registros.\n",
    "# Hago 20 iteraciones.\n",
    "# Valido con el 20% de los datos.\n",
    "modelo = crearModelo()\n",
    "historia = modelo.fit(train_matriz_x, train_y, batch_size = 32, epochs = 20, validation_split = 0.33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El puntaje logrado por este algoritmo fue de:  0.48852941393852234\n"
     ]
    }
   ],
   "source": [
    "predicciones = modelo.predict(test_matriz_x)\n",
    "categorias = array.array('i')\n",
    "\n",
    "for prediccion in predicciones:\n",
    "    categorias.append(np.argmax(prediccion))\n",
    "    \n",
    "# Genero un dataframe de pandas y lo guardo.\n",
    "test_prediccion = test_df['id'].to_frame()\n",
    "test_prediccion['target'] = pd.Series(categorias)\n",
    "test_prediccion.to_csv('tensorFlow.csv', index = False)\n",
    "\n",
    "precision = historia.history[modelo.metrics_names[2]][-1]\n",
    "recall = historia.history[modelo.metrics_names[3]][-1]\n",
    "f1 = precision * recall / (precision + recall)\n",
    "print('El puntaje logrado por este algoritmo fue de: ', f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizacion del clasificador de texto básico\n",
    "En este notebook se buscaran parámetros que optimicen al clasificador. <br>\n",
    "Recordemos que solo se utiliza el texto para realizar la clasificación. <br>\n",
    "Como fuimos nombrando a lo largo de este notebook los siguientes son los parámetros que podríamos optimizar:\n",
    "- 1 - Cantidad de palabras máximas.\n",
    "- 2 - Número de salida del primer layer.\n",
    "- 3 - Función de activación del primer layer.\n",
    "- 4 - El porcentage de desactivación del primer dropout.\n",
    "- 5 - Número de salida del segundo layer.\n",
    "- 6 - Función de activación del segundo layer.\n",
    "- 7 - El porcentage de desactivación del segundo dropout.\n",
    "- 8 - Función de activación del layer final.\n",
    "- 9 - La función loss del modelo.\n",
    "- 10 - La función optimizer del modelo.\n",
    "- 11 - Batch_size.\n",
    "- 12 - Epochs.\n",
    "- 13 - Split.<br>\n",
    "\n",
    "Sin embargo para este ejemplo solo nos vamos a concentrar en optimizar aquellos parámetros que son necesarios para realizar el fit, para ello se hará una especie de gridsearch con los parámetros batch_size, epochs y validation_split.<br> Para cada parámetro se dejaran fijos con los valores default los demás y se buscara el valor que optimice el fit.<br>\n",
    "Los valores default para los parámetros a optimizar son:<br>\n",
    "- batch_size = 32.\n",
    "- epochs = 1.\n",
    "- validation_split = 0.0.\n",
    "\n",
    "Una vez que se obtengan los parámetros que maximizan este train, se hará predicciones sobre el test.\n",
    "Claramente esto dará un overfitting bastante marcado ya que se esta sobreentrenando el modelo con los datos ingresados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización del parámetro validation_split.\n",
    "def optimizacionValidacion(batch_size, epochs):\n",
    "    validacion = 0.01\n",
    "    validacionDelta = 0.01\n",
    "\n",
    "    mejor_resultado = 0\n",
    "    mejor_validacion = 0.1\n",
    "    \n",
    "    modelo_val_opt = crearModelo()\n",
    "\n",
    "    while validacion < 1.0:\n",
    "        print('#################################### Validación: ', validacion, ' ####################################')\n",
    "        historia_val_opt = modelo_val_opt.fit(train_matriz_x, train_y, batch_size = batch_size, epochs = epochs, validation_split = validacion, verbose = 0)\n",
    "    \n",
    "        precision_val_opt = historia_val_opt.history[modelo_val_opt.metrics_names[2]][-1]\n",
    "        recall_val_opt = historia_val_opt.history[modelo_val_opt.metrics_names[3]][-1]\n",
    "        f1_val_opt = 2 * (precision_val_opt * recall_val_opt / (precision_val_opt + recall_val_opt))\n",
    "        acc_val_opt = historia_val_opt.history[modelo_val_opt.metrics_names[1]][-1]\n",
    "        \n",
    "        print('Validación: ', validacion, '; Puntaje: ', f1_val_opt)\n",
    "    \n",
    "        if f1_val_opt > mejor_resultado:\n",
    "            mejor_resultado = f1_val_opt\n",
    "            mejor_validacion = validacion\n",
    "            print('Mejor resultado para el split: ', validacion)\n",
    "        \n",
    "        if acc_val_opt == 1:\n",
    "            return mejor_resultado, mejor_validacion\n",
    "        \n",
    "        validacion = validacion + validacionDelta\n",
    "    \n",
    "    return mejor_resultado, mejor_validacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización del parámetro epochs.\n",
    "def optimizacionEpochs(batch_size, validation_split):\n",
    "    epochs_value = 1\n",
    "    epochsDelta = 1\n",
    "    \n",
    "    mejor_resultado = 0\n",
    "    mejor_epochs = 1\n",
    "    \n",
    "    modelo_epochs_opt = crearModelo()\n",
    "    \n",
    "    while epochs_value <= 25:\n",
    "        print('#################################### Epochs: ', epochs_value, ' ####################################')\n",
    "        historia_epochs_opt = modelo_epochs_opt.fit(train_matriz_x, train_y, batch_size = batch_size, epochs = epochs_value, validation_split = validation_split, verbose = 0)\n",
    "        \n",
    "        precision_epochs_opt = historia_epochs_opt.history[modelo_epochs_opt.metrics_names[2]][-1]\n",
    "        recall_epochs_opt = historia_epochs_opt.history[modelo_epochs_opt.metrics_names[3]][-1]\n",
    "        f1_epochs_opt = 2 * (precision_epochs_opt * recall_epochs_opt / (precision_epochs_opt + recall_epochs_opt))\n",
    "        acc_epochs_opt = historia_epochs_opt.history[modelo_epochs_opt.metrics_names[1]][-1]\n",
    "        \n",
    "        print('Epochs: ', epochs_value, '; Puntaje: ', f1_epochs_opt)\n",
    "        \n",
    "        if f1_epochs_opt > mejor_resultado:\n",
    "            mejor_resultado = f1_epochs_opt\n",
    "            mejor_epochs = epochs_value\n",
    "            print('Mejor resultado para el Epochs: ', epochs_value)\n",
    "        \n",
    "        if acc_epochs_opt == 1:\n",
    "            return mejor_resultado, mejor_epochs\n",
    "        \n",
    "        epochs_value = epochs_value + epochsDelta\n",
    "    \n",
    "    return mejor_resultado, mejor_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimización del parámetro bach_size.\n",
    "def optimizacionBatchSize(epochs, validation_split):\n",
    "    batch_size = 1\n",
    "    batch_size_delta = 2\n",
    "    \n",
    "    mejor_resultado = 0\n",
    "    mejor_batch = 1\n",
    "    \n",
    "    modelo_batch_opt = crearModelo()\n",
    "    \n",
    "    while batch_size <= len(train_y):\n",
    "        print('#################################### Batch: ', batch_size, ' ####################################')\n",
    "        historia_batch_opt = modelo_batch_opt.fit(train_matriz_x, train_y, batch_size = batch_size, epochs = epochs, validation_split = validacion, verbose = 0)\n",
    "        \n",
    "        precision_batch_opt = historia_batch_opt.history[modelo_batch_opt.metrics_names[2]][-1]\n",
    "        recall_batch_opt = historia_batch_opt.history[modelo_batch_opt.metrics_names[3]][-1]\n",
    "        f1_batch_opt = 2 * (precision_batch_opt * recall_batch_opt / (precision_batch_opt + recall_batch_opt))\n",
    "        acc_batch_opt = historia_batch_opt.history[modelo_batch_opt.metrics_names[1]][-1]\n",
    "        \n",
    "        print('Batch: ', batch_size, '; Puntaje: ', f1_batch_opt)\n",
    "        \n",
    "        if f1_batch_opt > mejor_resultado:\n",
    "            mejor_resultado = f1_batch_opt\n",
    "            mejor_batch = batch_size\n",
    "            print('Mejor resultado para el Batch: ', batch_size)\n",
    "            \n",
    "        if acc_batch_opt == 1:\n",
    "            return mejor_resultado, batch_size\n",
    "        \n",
    "        if batch_size > 100:\n",
    "            batch_size = batch_size + 100\n",
    "        else:\n",
    "            batch_size = batch_size * batch_size_delta\n",
    "    \n",
    "    return mejor_resultado, batch_size\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################################### Validación:  0.01  ####################################\n",
      "Validación:  0.01 ; Puntaje:  0.743763267993927\n",
      "Mejor resultado para el split:  0.01\n",
      "#################################### Validación:  0.02  ####################################\n",
      "Validación:  0.02 ; Puntaje:  0.8644772171974182\n",
      "Mejor resultado para el split:  0.02\n",
      "#################################### Validación:  0.03  ####################################\n",
      "Validación:  0.03 ; Puntaje:  0.9135969877243042\n",
      "Mejor resultado para el split:  0.03\n",
      "#################################### Validación:  0.04  ####################################\n",
      "Validación:  0.04 ; Puntaje:  0.951286256313324\n",
      "Mejor resultado para el split:  0.04\n",
      "#################################### Validación:  0.05  ####################################\n"
     ]
    }
   ],
   "source": [
    "# Empiezo el timer.\n",
    "tiempo_comienzo = time.time()\n",
    "\n",
    "# Valores default.\n",
    "default_batch_size = 32\n",
    "default_epochs = 1\n",
    "default_split = 0.0\n",
    "\n",
    "mejor_resultado = 0\n",
    "mejor_batch_size = 32\n",
    "mejor_epochs = 1\n",
    "mejor_split = 0.0\n",
    "\n",
    "# Optimizaciones.\n",
    "# 1 - Con los demás parámetros en default se buscará el mejor batch_size, epochs y validation_split.\n",
    "# Con esto obtengo los primeros valores de batch_size, epochs y validation_split para empezar a trabajar.\n",
    "mejor_resultado_val, validacion = optimizacionValidacion(default_batch_size, default_epochs)\n",
    "mejor_split = validacion\n",
    "    \n",
    "mejor_resultado_epochs, epochs = optimizacionEpochs(default_batch_size, default_split)\n",
    "mejor_epochs = epochs\n",
    "    \n",
    "mejor_resultado_batch, batch_size = optimizacionBatchSize(default_epochs, default_split)\n",
    "mejor_batch_size = batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Con los datos obtenidos hacemos las predicciones.\n",
    "modelo_predictor = crearModelo()\n",
    "modelo_predictor.fit(train_matriz_x, train_y, batch_size = mejor_batch_size, epochs = mejor_epochs, validation_split = mejor_split)\n",
    "modelo_predic = modelo_predictor.predict(test_matriz_x)\n",
    "modelo_categorias = array.array('i')\n",
    "\n",
    "for prediccion in modelo_predic:\n",
    "    modelo_categorias.append(np.argmax(prediccion))\n",
    "    \n",
    "# Genero un dataframe de pandas y lo guardo.\n",
    "modelo_predic_df = test_df['id'].to_frame()\n",
    "modelo_predic_df['target'] = pd.Series(modelo_categorias)\n",
    "modelo_predic_df.to_csv('tensorFlow-opt.csv', index = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
