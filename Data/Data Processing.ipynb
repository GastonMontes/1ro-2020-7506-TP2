{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import STOPWORDS\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print (train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de la Columna Text\n",
    "\n",
    "- Se elimina enlaces y espacios en blanco innecesarios\n",
    "- Se crea columnas separadas que contengan listas de hashtags, menciones y enlaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'https?://\\S+', '', text) # Se elimina link\n",
    "    text = re.sub(r'\\n',' ', text) # Se elimina saltos de linea\n",
    "    text = re.sub('\\s+', ' ', text).strip() # Se eliminan espacios iniciales, finales y adicionales\n",
    "    return text\n",
    "\n",
    "def find_hashtags(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_mentions(tweet):\n",
    "    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"@\\w+\", tweet)]) or 'no'\n",
    "\n",
    "def find_links(tweet):\n",
    "    return \" \".join([match.group(0)[:] for match in re.finditer(r\"https?://\\S+\", tweet)]) or 'no'\n",
    "\n",
    "def process_text(df):\n",
    "    df['text_clean'] = df['text'].apply(lambda x: clean_text(x))\n",
    "    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n",
    "    df['mentions'] = df['text'].apply(lambda x: find_mentions(x))\n",
    "    df['links'] = df['text'].apply(lambda x: find_links(x))    \n",
    "    return df\n",
    "\n",
    "train_processed = train.copy()\n",
    "test_processed = test.copy()\n",
    "train_processed = process_text(train_processed)\n",
    "test_processed = process_text(test_processed)\n",
    "\n",
    "print(train_processed.shape, test_processed.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se crean estadisticas a partir de la Columna text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crear_estadistica(df):\n",
    "    # Longitud del Tweet\n",
    "    df['text_len'] = df['text_clean'].apply(len)\n",
    "    # Cantidad de Palabras\n",
    "    df['word_count'] = df[\"text_clean\"].apply(lambda x: len(str(x).split()))\n",
    "    # Cantidad de palabras unicas\n",
    "    df['unique_word_count'] = df['text_clean'].apply(lambda x:len(set(str(x).split())))\n",
    "    # Longitud de palabras promedio\n",
    "    df['mean_word_length'] = df['text_clean'].apply(lambda x: np.mean([len(word) for word in str(x).split()]))\n",
    "    # Cantidad de caracteres\n",
    "    df['character_count'] = df['text_clean'].apply(lambda x:len(str(x)))\n",
    "    # Cantidad de digitos\n",
    "    df['digit_count'] = df['text_clean'].apply(lambda x: np.sum([len(char) for char in x if char.isdigit() == True]))\n",
    "    # Cantidad de Stopword\n",
    "    df['stop_word_count'] = df['text_clean'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    # Cantidad de Puntuaciones (.)\n",
    "    df['punctuation_count'] = df['text_clean'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    # Cantidad de Hashtags (#)\n",
    "    df['hashtag_count'] = df['hashtags'].apply(lambda x: len(str(x).split()))\n",
    "    # Cantidad de Menciones (@)\n",
    "    df['mention_count'] = df['mentions'].apply(lambda x: len(str(x).split()))\n",
    "    # Cantidad de Links\n",
    "    df['link_count'] = df['links'].apply(lambda x: len(str(x).split()))\n",
    "    # Cantidad de letras Mayusculas\n",
    "    df['caps_count'] = df['text_clean'].apply(lambda x: sum(1 for c in str(x) if c.isupper()))\n",
    "    # Proporcion de letras Mayusculas\n",
    "    df['caps_ratio'] = df['caps_count'] / df['text_len']\n",
    "    return df\n",
    "\n",
    "train_processed = crear_estadistica(train_processed)\n",
    "test_processed = crear_estadistica(test_processed)\n",
    "\n",
    "print(train_processed.shape, test_processed.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de la Columna Location\n",
    "\n",
    "- Se une las localizaciones equivalentes\n",
    "- Dejamos las localizaciones que estan en el Top 10 y es resto lo agrupamos en Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NA values\n",
    "raw_loc = train_processed.location.value_counts()\n",
    "top_loc = list(raw_loc[raw_loc>=10].index)\n",
    "\n",
    "for col in ['keyword','location']:\n",
    "    train_processed[col] = train_processed[col].fillna('None')\n",
    "    test_processed[col] = test_processed[col].fillna('None')\n",
    "\n",
    "def clean_loc(x):\n",
    "    if x == 'None':\n",
    "        return 'None'\n",
    "    elif x == 'Earth' or x =='Worldwide' or x == 'Everywhere':\n",
    "        return 'World'\n",
    "    elif 'New York' in x or 'NYC' in x:\n",
    "        return 'New York'    \n",
    "    elif 'London' in x:\n",
    "        return 'London'\n",
    "    elif 'Mumbai' in x:\n",
    "        return 'Mumbai'\n",
    "    elif 'Washington' in x and 'D' in x and 'C' in x:\n",
    "        return 'Washington DC'\n",
    "    elif 'San Francisco' in x:\n",
    "        return 'San Francisco'\n",
    "    elif 'Los Angeles' in x:\n",
    "        return 'Los Angeles'\n",
    "    elif 'Seattle' in x:\n",
    "        return 'Seattle'\n",
    "    elif 'Chicago' in x:\n",
    "        return 'Chicago'\n",
    "    elif 'Toronto' in x:\n",
    "        return 'Toronto'\n",
    "    elif 'Sacramento' in x:\n",
    "        return 'Sacramento'\n",
    "    elif 'Atlanta' in x:\n",
    "        return 'Atlanta'\n",
    "    elif 'California' in x:\n",
    "        return 'California'\n",
    "    elif 'Florida' in x:\n",
    "        return 'Florida'\n",
    "    elif 'Texas' in x:\n",
    "        return 'Texas'\n",
    "    elif 'United States' in x or 'USA' in x:\n",
    "        return 'USA'\n",
    "    elif 'United Kingdom' in x or 'UK' in x or 'Britain' in x:\n",
    "        return 'UK'\n",
    "    elif 'Canada' in x:\n",
    "        return 'Canada'\n",
    "    elif 'India' in x:\n",
    "        return 'India'\n",
    "    elif 'Kenya' in x:\n",
    "        return 'Kenya'\n",
    "    elif 'Nigeria' in x:\n",
    "        return 'Nigeria'\n",
    "    elif 'Australia' in x:\n",
    "        return 'Australia'\n",
    "    elif 'Indonesia' in x:\n",
    "        return 'Indonesia'\n",
    "    elif x in top_loc:\n",
    "        return x\n",
    "    else: return 'Others'\n",
    "    \n",
    "train_processed['location_clean'] = train_processed['location'].apply(lambda x: clean_loc(str(x)))\n",
    "test_processed['location_clean'] = test_processed['location'].apply(lambda x: clean_loc(str(x)))\n",
    "\n",
    "print(train_processed.shape, test_processed.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_l2 = train_processed.groupby('location_clean').mean()['target'].sort_values(ascending=False)\n",
    "plt.figure(figsize=(14,6))\n",
    "sns.barplot(x=top_l2.index, y=top_l2)\n",
    "plt.axhline(np.mean(train_processed.target))\n",
    "plt.xticks(rotation=80)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Codificacion de variables categoricas\n",
    "\n",
    "Como parte de la generacion de feactures haremos lo siguiente\n",
    "- Aplicar target encoding para las columnas keyword y location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "# Target encoding\n",
    "features = ['keyword', 'location_clean']\n",
    "encoder = ce.TargetEncoder(cols=features)\n",
    "encoder.fit(train_processed[features],train_processed['target'])\n",
    "\n",
    "train_processed = train_processed.join(encoder.transform(train_processed[features]).add_suffix('_target'))\n",
    "test_processed = test_processed.join(encoder.transform(test_processed[features]).add_suffix('_target'))\n",
    "\n",
    "print(train_processed.shape, test_processed.shape)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words\n",
    "\n",
    "- Se aplica countVectorizer el cual genera una columna por palabra y cuenta la frecuencia de cada palabra\n",
    "- Se aplica Bag of Words a las columnas Links, Mentions, Hashtags "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only include those >=5 occurrences\n",
    "\n",
    "# Links\n",
    "vec_links = CountVectorizer(min_df = 5, analyzer = 'word', token_pattern = r'https?://\\S+')\n",
    "link_vec = vec_links.fit_transform(train_processed['links'])\n",
    "link_vec_test = vec_links.transform(test_processed['links'])\n",
    "X_train_link = pd.DataFrame(link_vec.toarray(), columns=vec_links.get_feature_names())\n",
    "X_test_link = pd.DataFrame(link_vec_test.toarray(), columns=vec_links.get_feature_names())\n",
    "\n",
    "# Mentions\n",
    "vec_men = CountVectorizer(min_df = 5)\n",
    "men_vec = vec_men.fit_transform(train_processed['mentions'])\n",
    "men_vec_test = vec_men.transform(test_processed['mentions'])\n",
    "X_train_men = pd.DataFrame(men_vec.toarray(), columns=vec_men.get_feature_names())\n",
    "X_test_men = pd.DataFrame(men_vec_test.toarray(), columns=vec_men.get_feature_names())\n",
    "\n",
    "# Hashtags\n",
    "vec_hash = CountVectorizer(min_df = 5)\n",
    "hash_vec = vec_hash.fit_transform(train_processed['hashtags'])\n",
    "hash_vec_test = vec_hash.transform(test_processed['hashtags'])\n",
    "X_train_hash = pd.DataFrame(hash_vec.toarray(), columns=vec_hash.get_feature_names())\n",
    "X_test_hash = pd.DataFrame(hash_vec_test.toarray(), columns=vec_hash.get_feature_names())\n",
    "\n",
    "train_bow = train_processed.join(X_train_link, rsuffix='_link')\n",
    "train_bow = train_bow.join(X_train_men, rsuffix='_mention')\n",
    "train_bow = train_bow.join(X_train_hash, rsuffix='_hashtag')\n",
    "\n",
    "test_bow = test_processed.join(X_test_link, rsuffix='_link')\n",
    "test_bow = test_bow.join(X_test_men, rsuffix='_mention')\n",
    "test_bow = test_bow.join(X_test_hash, rsuffix='_hashtag')\n",
    "\n",
    "print (train_bow.shape, test_bow.shape)\n",
    "print(train_processed.shape, test_processed.shape)\n",
    "print (train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se aplica TF-IDF a la columna Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tf-idf for text\n",
    "\n",
    "# Only include >=10 occurrences\n",
    "# Have unigrams and bigrams\n",
    "vec_text = TfidfVectorizer(min_df = 10, ngram_range = (1,2), stop_words='english') \n",
    "text_vec = vec_text.fit_transform(train_processed['text_clean'])\n",
    "text_vec_test = vec_text.transform(test_processed['text_clean'])\n",
    "X_train_text = pd.DataFrame(text_vec.toarray(), columns=vec_text.get_feature_names())\n",
    "X_test_text = pd.DataFrame(text_vec_test.toarray(), columns=vec_text.get_feature_names())\n",
    "\n",
    "train_tf_idf = train.join(X_train_text, rsuffix='_text')\n",
    "test_tf_idf = test.join(X_test_text, rsuffix='_text')\n",
    "\n",
    "train_tf_idf_bow = train_bow.join(X_train_text, rsuffix='_text')\n",
    "test_tf_idf_bow = test_bow.join(X_test_text, rsuffix='_text')\n",
    "\n",
    "print (train_tf_idf_bow.shape, test_tf_idf_bow.shape)\n",
    "print (train_tf_idf.shape, test_tf_idf.shape)\n",
    "print (train_bow.shape, test_bow.shape)\n",
    "print(train_processed.shape, test_processed.shape)\n",
    "print (train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Se guardan los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed.to_csv('train_processed.csv')\n",
    "test_processed.to_csv('test_processed.csv')\n",
    "\n",
    "train_bow.to_csv('train_processed_bow.csv')\n",
    "test_bow.to_csv('test_processed_bow.csv')\n",
    "\n",
    "train_tf_idf.to_csv('train_processed_tf_idf.csv')\n",
    "test_tf_idf.to_csv('test_processed_tf_idf.csv')\n",
    "\n",
    "train_tf_idf_bow.to_csv('train_processed_tf_idf_bow.csv')\n",
    "test_tf_idf_bow.to_csv('test_processed_tf_idf_bow.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
